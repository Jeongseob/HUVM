diff --git a/nvidia-uvm/nvidia-uvm.Kbuild b/nvidia-uvm/nvidia-uvm.Kbuild
index 7501eed..73a01ed 100644
--- a/nvidia-uvm/nvidia-uvm.Kbuild
+++ b/nvidia-uvm/nvidia-uvm.Kbuild
@@ -2,7 +2,7 @@
 # Kbuild fragment for nvidia-uvm.ko
 ###########################################################################
 
-UVM_BUILD_TYPE = release
+UVM_BUILD_TYPE = debug
 
 #
 # Define NVIDIA_UVM_{SOURCES,OBJECTS}
diff --git a/nvidia-uvm/uvm_gpu_replayable_faults.c b/nvidia-uvm/uvm_gpu_replayable_faults.c
index 97cbf37..a7af711 100644
--- a/nvidia-uvm/uvm_gpu_replayable_faults.c
+++ b/nvidia-uvm/uvm_gpu_replayable_faults.c
@@ -2629,11 +2629,15 @@ static NV_STATUS service_parallel_fault(uvm_parallel_fetch_context_t *parallel_f
 
         va_range = uvm_va_range_find(va_space, fault_address);
 
-        if (!va_range || va_range->type != UVM_VA_RANGE_TYPE_MANAGED)
-            continue;
+        if (!va_range || va_range->type != UVM_VA_RANGE_TYPE_MANAGED) {
+            status = NV_ERR_INVALID_ADDRESS;
+            break;
+        }
 
-        if (uvm_va_block_find(va_space, fault_address, &va_block) != NV_OK)
-            continue;
+        if (uvm_va_block_find(va_space, fault_address, &va_block) != NV_OK) {
+            status = NV_ERR_INVALID_ADDRESS;
+            break;
+        }
 
         service_parallel_fault_in_block(va_block, parallel_fetch_context->final_dst_gpu);
 
@@ -2718,6 +2722,8 @@ static NV_STATUS classify_va_block_to_prefetch(const uvm_prefetch_context_t *pre
 // Return codes:
 // - NV_OK if prefetch required && parallel prefetch scheduled
 // - NV_ERR_IN_USE if get kill signal from the caller thread
+// - NV_OK if prefetch required && parallel prefetch scheduled
+// - NV_ERR_IN_USE if get kill signal from the caller thread
 //                 (implies no parallel prefetch scheduled)
 // - NV_WARN_NOTHING_TO_DO if no more any type of prefetch required
 // - NV_ERR_CALLBACK_NOT_SCHEDULED if prefetch required but no parallel
diff --git a/nvidia-uvm/uvm_pmm_gpu.c b/nvidia-uvm/uvm_pmm_gpu.c
index 98d024f..50313a6 100644
--- a/nvidia-uvm/uvm_pmm_gpu.c
+++ b/nvidia-uvm/uvm_pmm_gpu.c
@@ -181,7 +181,7 @@ MODULE_PARM_DESC(uvm_global_oversubscription, "Enable (1) or disable (0) global
 
 //TSKIM
 // Threshold memory count rate (out of 100)
-#define UVM_ENOUGH_MEM_THRESHOLD_RATE 90
+#define UVM_ENOUGH_MEM_THRESHOLD_RATE 85
 
 //TSKIM
 // Make easier to calculate free chunks to reserve.
@@ -205,6 +205,7 @@ module_param(uvm_reserve_chunk_level, uint, S_IRUGO);
 static unsigned uvm_perf_pma_batch_nonpinned_order = UVM_PERF_PMA_BATCH_NONPINNED_ORDER_DEFAULT;
 module_param(uvm_perf_pma_batch_nonpinned_order, uint, S_IRUGO);
 
+
 // Helper type for refcounting cache
 typedef struct
 {
@@ -1819,17 +1820,19 @@ void uvm_pmm_gpu_mark_root_chunk_unused(uvm_pmm_gpu_t *pmm, uvm_gpu_chunk_t *chu
 static uvm_gpu_root_chunk_t *pick_root_chunk_to_evict(uvm_pmm_gpu_t *pmm, uvm_pmm_alloc_flags_t flags)
 {
     uvm_gpu_chunk_t *chunk;
+    NvU32 retry_cnt = 0;
 
+retry:
     uvm_spin_lock(&pmm->list_lock);
 
     // Check if there are root chunks sitting in the free lists. Non-zero
     // chunks are preferred.
-    chunk = list_first_chunk(find_free_list(pmm,
-                                            UVM_PMM_GPU_MEMORY_TYPE_USER,
-                                            UVM_CHUNK_SIZE_MAX,
-                                            UVM_PMM_LIST_NO_ZERO));
-    if (chunk)
-        UVM_ASSERT(!chunk->is_zero);
+	chunk = list_first_chunk(find_free_list(pmm,
+										UVM_PMM_GPU_MEMORY_TYPE_USER,
+										UVM_CHUNK_SIZE_MAX,
+										UVM_PMM_LIST_NO_ZERO));
+	if (chunk)
+		UVM_ASSERT(!chunk->is_zero);
 
     if (!chunk) {
         chunk = list_first_chunk(find_free_list(pmm,
@@ -1846,28 +1849,34 @@ static uvm_gpu_root_chunk_t *pick_root_chunk_to_evict(uvm_pmm_gpu_t *pmm, uvm_pm
     // "eviction" because both of following reasons.
     // - REMOVABLE chunk should be also available on PMA eivction.
     // - It erase existing contents of chunk, same as other traditional eviction.
-    if (!chunk)
+    if (!chunk) 
         chunk = list_first_chunk(find_removable_list(pmm, UVM_CHUNK_SIZE_MAX));
 
     if (!chunk)
         chunk = list_first_chunk(find_parallel_copy_list(pmm, UVM_CHUNK_SIZE_MAX));
 
-    if (flags & UVM_PMM_ALLOC_FLAGS_ONLY_REMOVABLE)
+    if (flags & UVM_PMM_ALLOC_FLAGS_ONLY_REMOVABLE) {
+        if (!chunk) {
+			if (list_first_chunk(find_evicted_list(pmm, UVM_CHUNK_SIZE_MAX))) {
+                if (retry_cnt++ < 1000000) {
+    			    uvm_spin_unlock(&pmm->list_lock);
+				    goto retry;
+                }
+            }
+        }
         goto out;
+    }
 
     if (!chunk)
         chunk = list_first_chunk(&pmm->root_chunks.va_block_unused);
 
-    /*
-    if (!chunk)
-        chunk = list_first_chunk(find_evicted_list(pmm, UVM_CHUNK_SIZE_MAX));
-        */
-
     // TODO: Bug 1765193: Move the chunks to the tail of the used list whenever
     // they get mapped.
     if (!chunk)
         chunk = list_first_chunk(&pmm->root_chunks.va_block_used);
 
+    if (!chunk)
+        chunk = list_first_chunk(find_evicted_list(pmm, UVM_CHUNK_SIZE_MAX));
 out:
     if (chunk)
         chunk_start_eviction(pmm, chunk);
@@ -1898,6 +1907,17 @@ static NV_STATUS pick_and_evict_root_chunk(uvm_pmm_gpu_t *pmm,
     if (!root_chunk)
         return NV_ERR_NO_MEMORY;
 
+    /*
+    if (uvm_pmm_gpu_has_enough_mem_space(pmm)) {
+        pmm->can_harvest = true;
+        printk("can harvest %d\n", pmm->gpu->id.val);
+    }
+    else {
+        pmm->can_harvest = false;
+        //printk("cannot harvest %d\n", pmm->gpu->id.val);
+    }
+    */
+
     status = evict_root_chunk(pmm, root_chunk, pmm_context);
     if (status != NV_OK)
         return status;
@@ -4032,7 +4052,7 @@ static NvU32 pmm_get_active_memory_rate(const uvm_pmm_gpu_t *pmm)
 }
 
 //TSKIM
-static bool uvm_pmm_gpu_has_enough_mem_space(const uvm_pmm_gpu_t *pmm)
+bool uvm_pmm_gpu_has_enough_mem_space(const uvm_pmm_gpu_t *pmm)
 {
     // Percentage
     return pmm_get_active_memory_rate(pmm) < UVM_ENOUGH_MEM_THRESHOLD_RATE;
@@ -4070,7 +4090,7 @@ static NV_STATUS clear_removable_chunk_from_va_block(uvm_pmm_gpu_t *pmm, uvm_gpu
     UVM_ASSERT(va_block);
     uvm_mutex_lock(&va_block->lock);
 
-    status = uvm_va_block_clear_removable_resident(va_block, pmm->gpu, &tracker);
+    status = uvm_va_block_clear_removable_resident(va_block, chunk, pmm->gpu, &tracker);
 
     uvm_mutex_unlock(&va_block->lock);
 
@@ -4249,22 +4269,24 @@ static void pre_evict_chunk(uvm_pmm_gpu_t *pmm)
     uvm_gpu_chunk_t *chunk = NULL;
     NV_STATUS status;
 
-    uvm_mutex_lock(&pmm->lock);
-
     //TSKIM
     do {
-        root_chunk = pick_root_chunk_to_pre_evict(pmm);
-        if (!root_chunk)
-            break;
+    root_chunk = pick_root_chunk_to_pre_evict(pmm);
+    if (!root_chunk) {
+        return;
+    }
 
-        status = evict_root_chunk(pmm, root_chunk, PMM_CONTEXT_DEFAULT);
-        if (status != NV_OK)
-            break;
+    uvm_mutex_lock(&pmm->lock);
+    status = evict_root_chunk(pmm, root_chunk, PMM_CONTEXT_DEFAULT);
+    uvm_mutex_unlock(&pmm->lock);
 
-        chunk = &root_chunk->chunk;
+    if (status != NV_OK) {
+        return;
+    } 
+
+    chunk = &root_chunk->chunk;
     } while (status == NV_ERR_IN_USE);
 
-    uvm_mutex_unlock(&pmm->lock);
 
     if (chunk) {
         uvm_spin_lock(&pmm->list_lock);
@@ -4285,7 +4307,6 @@ static void reserve_free_chunk(void *args)
 
     uvm_gpu_retain(gpu);
 
-
     do {
         if (uvm_gpu_get_num_free_chunks(pmm) > UVM_NUM_CHUNK_TO_RESERVE(pmm->reserve_level)) {
             goto out;
@@ -4294,9 +4315,9 @@ static void reserve_free_chunk(void *args)
             status = pre_alloc_root_chunk(pmm, UVM_PMM_GPU_MEMORY_TYPE_USER, UVM_PMM_ALLOC_FLAGS_NONE);
     } while(status == NV_OK);
 
-    for (i = 0; i < UVM_NUM_CHUNK_TO_RESERVE(pmm->reserve_level) - pmm->num_free_chunks; i++)
+    for (i = 0; i < UVM_NUM_CHUNK_TO_RESERVE(pmm->reserve_level) - pmm->num_free_chunks; i++) {
         pre_evict_chunk(pmm);
-
+    }
 out:
     uvm_gpu_release(gpu);
 }
diff --git a/nvidia-uvm/uvm_pmm_gpu.h b/nvidia-uvm/uvm_pmm_gpu.h
index 5da7884..8fdb0ae 100644
--- a/nvidia-uvm/uvm_pmm_gpu.h
+++ b/nvidia-uvm/uvm_pmm_gpu.h
@@ -745,4 +745,5 @@ NvU32 uvm_gpu_reserve_and_get_num_free_chunks(uvm_gpu_t *gpu);
 //TSKIM
 NvU32 uvm_gpu_get_num_free_chunks(uvm_pmm_gpu_t *pmm);
 
+bool uvm_pmm_gpu_has_enough_mem_space(const uvm_pmm_gpu_t *pmm);
 #endif
diff --git a/nvidia-uvm/uvm_processors.h b/nvidia-uvm/uvm_processors.h
index d506c43..161d766 100644
--- a/nvidia-uvm/uvm_processors.h
+++ b/nvidia-uvm/uvm_processors.h
@@ -511,7 +511,7 @@ static uvm_processor_id_t uvm_gpu_id_round_robin(uvm_processor_id_t id, const uv
     uvm_processor_id_t round_robin_id = id;
     uvm_processor_id_t next_id;
 
-    for (i = 0; i < seed; i++) {
+    for (i = 0; i <= seed; i++) {
        next_id = uvm_processor_mask_find_next_id(mask, uvm_gpu_id_next(round_robin_id));
        if (UVM_ID_IS_INVALID(next_id))
            round_robin_id = uvm_processor_mask_find_first_gpu_id(mask);
diff --git a/nvidia-uvm/uvm_va_block.c b/nvidia-uvm/uvm_va_block.c
index d6be21a..36a33da 100644
--- a/nvidia-uvm/uvm_va_block.c
+++ b/nvidia-uvm/uvm_va_block.c
@@ -11145,18 +11145,11 @@ NV_STATUS uvm_va_block_evict_chunks(uvm_va_block_t *va_block,
         NvU8 eviction_seed;
         uvm_processor_id_t round_robin_dst_id;
     
-        //SJCHOI 
-        // if gpu A evicted to a neighbor gpu, other gpus should not harvest gpu A's memory.
-        // There is always free memory in gpu A because of pre-evict and other gpus should 
-        // not harvest that pre-evicted memory. I cannot think of a solution that can distinguish
-        // free memory that can be harvested and free memory caused by reserving free chunks. 
-        pmm->can_harvest = false;
-
         //SJCHOI
         // change round robin seed
         eviction_seed = pmm->eviction_seed;
         pmm->eviction_seed += 1;
-        if (UVM_GLOBAL_ID_IS_INVALID(pmm->eviction_seed))
+        if (pmm->eviction_seed > UVM_ID_MAX_GPUS)
             pmm->eviction_seed = 0;
 
         // Three cases in multi-GPU eviction (which_case)
@@ -11180,9 +11173,12 @@ NV_STATUS uvm_va_block_evict_chunks(uvm_va_block_t *va_block,
             if (uvm_processor_mask_test(&va_block->evicted_gpus, round_robin_dst_id))
                 continue;
 
-            //SJCHOI 
+			//SJCHOI 
             // Skip if dst_id cannot be harvested
-            if (!block_get_gpu(va_block, round_robin_dst_id)->pmm.can_harvest)
+            //if (!block_get_gpu(va_block, round_robin_dst_id)->pmm.can_harvest)
+                //continue;
+
+            if (!uvm_pmm_gpu_has_enough_mem_space(&block_get_gpu(va_block, round_robin_dst_id)->pmm))
                 continue;
 
             if (uvm_gpu_reserve_and_get_num_free_chunks(block_get_gpu(va_block, round_robin_dst_id)) > 0) {
@@ -11203,12 +11199,10 @@ NV_STATUS uvm_va_block_evict_chunks(uvm_va_block_t *va_block,
                                                                         UVM_MAKE_RESIDENT_CAUSE_EVICTION));
             // If target GPU is full and cannot remove removable page,
             // go to case 2. 
-            if (status == NV_ERR_NO_MEMORY) {
+            if (status != NV_OK) {
                 which_case = 2;
-            }
-            else if (status != NV_OK)
-                goto out;
-        }
+            } 
+	    }
         else { 
             // Failed to find first fit GPU go to case 2 to harvest removable page
             which_case = 2;
@@ -11216,6 +11210,9 @@ NV_STATUS uvm_va_block_evict_chunks(uvm_va_block_t *va_block,
 
         // If there is no empty GPU, find GPU that can replace removable pages.
         if (which_case == 2) {
+	        // SJCHOI debugging
+	        status = NV_ERR_NO_MEMORY;
+
             for_each_gpu_id_in_mask(dst_id, block_get_can_copy_from_mask(va_block, gpu->id)) {
                 
                 //SJCHOI
@@ -11229,10 +11226,13 @@ NV_STATUS uvm_va_block_evict_chunks(uvm_va_block_t *va_block,
                 // Skip if dest_id is set to evicted_gpu
                 if (uvm_processor_mask_test(&va_block->evicted_gpus, round_robin_dst_id))
                     continue;
+				
+				//SJCHOI 
+            	// Skip if dst_id cannot be harvested
+            	//if (!block_get_gpu(va_block, round_robin_dst_id)->pmm.can_harvest)
+                	//continue;
 
-                //SJCHOI
-                // Skip if dst_id cannot be harvested
-                if (!block_get_gpu(va_block, round_robin_dst_id)->pmm.can_harvest)
+                if (!uvm_pmm_gpu_has_enough_mem_space(&block_get_gpu(va_block, round_robin_dst_id)->pmm))
                     continue;
 
                 //TSKIM:TODO
@@ -11248,12 +11248,16 @@ NV_STATUS uvm_va_block_evict_chunks(uvm_va_block_t *va_block,
                                                                                 NULL,
                                                                                 UVM_MAKE_RESIDENT_CAUSE_EVICTION));
                 // Retry next gpu to harvest removable page (Still case 2)
-                if (status == NV_ERR_NO_MEMORY)
+                if (status == NV_ERR_NO_MEMORY) {
                     continue;
-                else if (status == NV_OK) // Sucess case 2 
+		        }
+                else if (status == NV_OK) { // Success case 2 
                     break;
-                else // Unexpected behavior
-                    goto out;
+		        }
+                else {
+                    which_case = 3;
+		            break;
+		        }
             }
         }
 
@@ -11264,10 +11268,11 @@ NV_STATUS uvm_va_block_evict_chunks(uvm_va_block_t *va_block,
             if (uvm_debug_counter_enabled())
                 atomic64_inc(&va_space->debug_counter[UVM_DEBUG_COUNTER_EVICTION_TO_REMOTE]);
         }
-        else 
+        else { 
             which_case = 3;
+	    }
     }
-
+	
     //TSKIM
     // If uvm_hierarchical_memory off or fail to evict to peer GPU, dst_id is
     // set to INVALID ID. Then, try evict to host.
@@ -11286,8 +11291,10 @@ NV_STATUS uvm_va_block_evict_chunks(uvm_va_block_t *va_block,
                 atomic64_inc(&va_space->debug_counter[UVM_DEBUG_COUNTER_EVICTION_TO_HOST]);
         }
     }
-    if (status != NV_OK)
+
+    if (status != NV_OK) {
         goto out;
+    }
 
     //TSKIM:Counter
     if (uvm_debug_counter_enabled())
@@ -11970,7 +11977,7 @@ error:
 }
 
 //TSKIM
-NV_STATUS uvm_va_block_clear_removable_resident(uvm_va_block_t *va_block, uvm_gpu_t *gpu, uvm_tracker_t *tracker)
+NV_STATUS uvm_va_block_clear_removable_resident(uvm_va_block_t *va_block, uvm_gpu_chunk_t *chunk, uvm_gpu_t *gpu, uvm_tracker_t *tracker)
 {
     uvm_va_block_gpu_state_t *gpu_state = block_gpu_state_get(va_block, gpu->id);
 
@@ -11985,7 +11992,7 @@ NV_STATUS uvm_va_block_clear_removable_resident(uvm_va_block_t *va_block, uvm_gp
     //TSKIM:EXPERIMENT
     // As I excepted mapping indirect peers when evicting, we don't have to unmap
     // indirect peers here.
-    // block_unmap_indirect_peers_from_gpu_chunk(block, gpu, chunk);
+    //block_unmap_indirect_peers_from_gpu_chunk(va_block, gpu, chunk);
 
     uvm_page_mask_zero(&gpu_state->resident);
     uvm_processor_mask_clear(&va_block->resident, gpu->id);
@@ -12055,8 +12062,8 @@ NV_STATUS uvm_va_block_duplicate_on_peer_locked(uvm_va_block_t *va_block,
     pmm = &block_get_gpu(va_block, final_dst_id)->pmm;
     parallel_fetch_seed = pmm->parallel_fetch_seed;
     pmm->parallel_fetch_seed += 1;
-    if (UVM_GLOBAL_ID_IS_INVALID(pmm->parallel_fetch_seed))
-        pmm->parallel_fetch_seed = 0;
+    if (pmm->parallel_fetch_seed > UVM_ID_MAX_GPUS)
+    	pmm->parallel_fetch_seed = 0;
 
     for_each_gpu_id_in_mask(copy_peer_id, block_get_can_copy_from_mask(va_block, final_dst_id)) {
 
@@ -12069,6 +12076,9 @@ NV_STATUS uvm_va_block_duplicate_on_peer_locked(uvm_va_block_t *va_block,
         if (uvm_processor_mask_test(&va_block->resident, round_robin_dst_id)) {
             goto out;
         }
+        
+        if (!uvm_pmm_gpu_has_enough_mem_space(&block_get_gpu(va_block, round_robin_dst_id)->pmm))
+            continue;
 
         pages_to_copy = uvm_va_block_resident_mask_get(va_block, UVM_ID_CPU);
         va_block_context->make_resident.dest_id = round_robin_dst_id;
diff --git a/nvidia-uvm/uvm_va_block.h b/nvidia-uvm/uvm_va_block.h
index 20ae5b6..677afc9 100644
--- a/nvidia-uvm/uvm_va_block.h
+++ b/nvidia-uvm/uvm_va_block.h
@@ -1901,7 +1901,7 @@ uvm_prot_t uvm_va_block_page_compute_highest_permission(uvm_va_block_t *va_block
 // Update va_block resident which was linked with gpu chunk.
 // Since removable gpu chunk is going to be freed and used to another
 // allocation, it just clears resident bit and related pages information.
-NV_STATUS uvm_va_block_clear_removable_resident(uvm_va_block_t *va_block, uvm_gpu_t *gpu, uvm_tracker_t *tracker);
+NV_STATUS uvm_va_block_clear_removable_resident(uvm_va_block_t *va_block, uvm_gpu_chunk_t *chunk, uvm_gpu_t *gpu, uvm_tracker_t *tracker);
 
 //TSKIM
 // Duplicate pages to peer GPUs as removable. The caller should release
